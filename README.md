Local LLM Chat is a lightweight web-based chatbot built with Streamlit that communicates directly with a locally installed Large Language Model (LLM) through Ollama.

With a simple and clean interface, you can chat with models like "llama2:latest" fully offline. The app includes a conversation history panel, a reset button, and uses a modern, responsive layout.

Key Features:
1. Chat in real-time with a local LLM â€” no cloud required
2. Conversation history stored in session
3. One-click reset for fresh chats
4. Simple to run and customize for your own models

Note: The code is AI Generated and this project is my first ever step into local LLMs. (I prefer honesty)
